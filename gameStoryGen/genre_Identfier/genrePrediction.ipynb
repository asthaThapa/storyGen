{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookDataset=pd.read_csv('data/bookDataset.csv')\n",
    "\n",
    "#Getting only those columns that seems necessary\n",
    "bookDataset=pd.DataFrame(bookDataset,columns=['genre','summary'])\n",
    "\n",
    "print(\"Raw dataset\")\n",
    "bookDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookDataset.groupby('genre').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Cleaning out inconsistent data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    if pd.isnull(text):  # Check if the text is NaN\n",
    "        return ''\n",
    "    text = re.sub(\"\\'\", \"\", text) \n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
    "    text = ' '.join(text.split()) \n",
    "    text = text.lower() \n",
    "    \n",
    "    return text\n",
    "\n",
    "bookDataset.loc[:,'summary']=bookDataset.loc[:,'summary'].apply(lambda x: clean(x))\n",
    "\n",
    "print(\"Cleaning out any character which is not an alphabet and converting all text to lowercase \\n\\n\")\n",
    "bookDataset['summary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Removing stop Words**\n",
    "\n",
    "Stop words such as \"the\", \"a\", \"an\" are assumed to have no impact on the over all classification process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mac user, use : others need to uncomment the following lines\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove stopwords\n",
    "def removestopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "bookDataset['summary'] = bookDataset['summary'].apply(lambda x: removestopwords(x))\n",
    "\n",
    "print(\"Removing Stop Words \\n\\n\")\n",
    "bookDataset['summary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Lemmatization of summary**\n",
    "\n",
    "Grouping of the different versions of the same word into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mac user, use : others need to uncomment the following lines\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "def lematizing(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = lemma.lemmatize(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "bookDataset['summary'] = bookDataset['summary'].apply(lambda x: lematizing(x))\n",
    "\n",
    "print(\"After Lematization \\n\\n\")\n",
    "bookDataset['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "\n",
    "bookDataset['summary'] = bookDataset['summary'].apply(lambda x: stemming(x))\n",
    "print(\"After Stemming \\n\\n\")\n",
    "bookDataset['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labeling each 'genre' with an unique number \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "\n",
    "y=LE.fit_transform(bookDataset['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform inverse mapping on the unique numbers representing a genre.\n",
    "\n",
    "LE.inverse_transform([0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_size 0.2 indicates that only 20% of the dataset is used for training while 80% will be used for testing\n",
    "xtrain, xval, ytrain, yval = train_test_split(bookDataset['summary'], y, test_size=0.15, random_state=246) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing tf-idf \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)\n",
    "\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain.values.astype('U'))\n",
    "\n",
    "xval_tfidf = tfidf_vectorizer.transform(xval.values.astype('U'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.now()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary Relevance.\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Performance metric.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = OneVsRestClassifier(lr)\n",
    "\n",
    "# fit model on train data.\n",
    "clf.fit(xtrain_tfidf, ytrain)\n",
    "\n",
    "# make predictions for validation set.\n",
    "y_pred_lr = clf.predict(xval_tfidf)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "  \n",
    "\n",
    "#Calculating the accuracy.\n",
    "print( 'Accuracy Score :',accuracy_score(yval,y_pred_lr) )\n",
    "\n",
    "#Printing the classification report.\n",
    "print ('Report : ')\n",
    "print(classification_report(yval,y_pred_lr))\n",
    "\n",
    "print(\"Executed in \",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=datetime.now()\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC(kernel='rbf',gamma=1).fit(xtrain_tfidf,ytrain)\n",
    "\n",
    "svpred=svc.predict(xval_tfidf)\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "  \n",
    "\n",
    "\n",
    "print( 'Accuracy Score :',accuracy_score(yval,svpred) )\n",
    "print ('Report : ')\n",
    "print(classification_report(yval,svpred))\n",
    "\n",
    "print(\"Executed in \",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Genre from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictGenre(q):\n",
    "    q = clean(q)\n",
    "    q = removestopwords(q)\n",
    "    q = lematizing(q)\n",
    "    q = stemming(q)\n",
    "    q_vec = tfidf_vectorizer.transform([q])\n",
    "    q_pred = svc.predict(q_vec)\n",
    "    return LE.inverse_transform(q_pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  story\n",
      "0     Once upon a time, in a warm and sunny place, t...\n",
      "1     Tom and Lily were playing with their toys in t...\n",
      "2     Once upon a time there was a little girl named...\n",
      "3     One morning, a cat named Tom woke up. He felt ...\n",
      "4     Lily and Tom were twins who liked to decorate ...\n",
      "...                                                 ...\n",
      "4995  Once upon a time, there was a girl named Mia. ...\n",
      "4996  Once upon a time, there was a furry tiger. The...\n",
      "4997  One day, a humble cat named Tom went for a wal...\n",
      "4998  Once upon a time, there was a little girl name...\n",
      "4999  Lily loved to play in the garden with her mom....\n",
      "\n",
      "[5000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#To read txt file, use read_fwf, to read csv use read_csv \n",
    "# rawDataSet =pd.read_csv('data/video_games.csv')\n",
    "rawDataSet = pd.read_fwf('data/TinyStoriesV3-GPT4-valid.txt')\n",
    "\n",
    "#We only need the first column, so get that and rename it for easier use\n",
    "rawDataSet = rawDataSet.iloc[:, :1]\n",
    "columnName = 'story' #Change the value as required \n",
    "# Since this data set did not have any column, it was assigned. Else you can use the existing column name\n",
    "rawDataSet.columns = [columnName]\n",
    "\n",
    "rawDataSubSet = rawDataSet.head(5000) #Can remove this later, there are 2 Million rows but just taking 5k from it for testing purpose\n",
    "rawDataSet = rawDataSubSet[columnName].copy()\n",
    "\n",
    "rawdf =pd.DataFrame(rawDataSet,columns=[columnName])\n",
    "print (rawdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total stories to process: 5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_stories = len(rawdf)\n",
    "print(f\"Total stories to process: {total_stories}\")\n",
    "\n",
    "rawdf['genre'] = rawdf[columnName].apply(predictGenre)\n",
    "\n",
    "# for index, row in rawdf.iterrows():\n",
    "#     story = row[columnName]\n",
    "#     genre = predictGenre(story)\n",
    "#     logging.info(f\"Processed story {index + 1}/{total_stories} - Predicted genre: {genre}\")\n",
    "\n",
    "#Make a new column for the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Horror' 'Fantasy' 'Historical novel' 'Crime Fiction' 'Science Fiction'\n",
      " 'Thriller']\n"
     ]
    }
   ],
   "source": [
    "# Need to know this genre so that it can be used in the generation code\n",
    "unique_genres = rawdf['genre'].unique()\n",
    "print(unique_genres)\n",
    "\n",
    "# Train = ['Fantasy' 'Thriller' 'Historical novel' 'Horror' 'Crime Fiction''Science Fiction']\n",
    "# Valid = ['Horror' 'Fantasy' 'Historical novel' 'Crime Fiction' 'Science Fiction', 'Thriller']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdf.to_csv('storyValid.txt', index=False)\n",
    "\n",
    "with open('storyTrain.txt', 'w') as file:\n",
    "    for index, row in rawdf.iterrows():\n",
    "        formatted_story = f\"<BOS> <{row['genre']}> {row['story']} <EOS>\\n\"\n",
    "        file.write(formatted_story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
